# critic_training.py

# Append the src directory to the path to find the 'noise' module
import sys
sys.path.append("./src")

from pathlib import Path
from typing import Dict, Any, List

import cv2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm
import numpy as np
import wandb

from noise.model.valuenet import ValueNetwork, compute_reward
from noise.utils.utils import collate_multimodal_batch

# ==============================================================================
# SECTION 1: DATASET HANDLING FOR REAL ROLLOUT DATA
# ==============================================================================

class TrajectoryDataset(Dataset):
    """
    A PyTorch Dataset specifically designed for loading trajectory data
    generated by your `rollout` script.

    This class handles loading .npy files, parsing the complex dictionary
    structure, and pre-calculating the discounted returns for each timestep,
    which will serve as the ground-truth labels for training the ValueNetwork.
    """

    def __init__(self, trajectory_files: List[Path], config: Dict[str, Any], history_len: int = 10):
        """
        Initializes the dataset by loading and processing all specified trajectories.

        Args:
            trajectory_files: A list of Path objects pointing to trajectory .npy files.
            config: A configuration dictionary containing hyperparameters.
            history_len: The length of the state-action history sequence for the GRU.
        """
        self.config = config
        self.history_len = history_len
        self.data_points = self._process_trajectories(trajectory_files)

    def _process_trajectories(self, trajectory_files: List[Path]) -> List[Dict[str, Any]]:
        """
        Iterates through trajectory files and converts them into individual
        (context, target_return) data points suitable for training.
        """
        all_data_points = []
        print(f"Loading and processing {len(trajectory_files)} trajectories...")

        for file_path in tqdm(trajectory_files, desc="Processing Trajectories"):
            if not file_path.exists():
                print(f"Warning: File not found, skipping: {file_path}")
                continue
            
            # Load the episode data from the .npy file
            # allow_pickle=True is necessary for loading dictionaries
            try:
                episode_data = np.load(file_path, allow_pickle=True)
            except Exception as e:
                print(f"Error loading {file_path}: {e}")
                continue

            # Extract the list of rewards for the entire trajectory
            # Your script saves the reward for the *noise model* directly
            rewards = [step['reward'] for step in episode_data]
            
            # Compute the discounted returns for the entire trajectory
            returns = self._compute_discounted_returns(rewards, self.config.get('gamma', 0.99))

            # Unroll the trajectory into individual data points
            for t in range(len(episode_data)):
                # --- Create state-action history ---
                # We take the `history_len` steps leading up to the current step `t`
                start_idx = max(0, t - self.history_len + 1)
                history_steps = episode_data[start_idx : t + 1]
                
                # Extract state and action from history
                states = np.array([step['state'] for step in history_steps])
                actions = np.array([step['action'] for step in history_steps])
                state_action_hist = np.concatenate([states, actions], axis=-1)
                
                # Pad if the history is shorter than `history_len` (at the start of an episode)
                if len(state_action_hist) < self.history_len:
                    padding_needed = self.history_len - len(state_action_hist)
                    padding = np.zeros((padding_needed, state_action_hist.shape[1]), dtype=np.float32)
                    state_action_hist = np.vstack([padding, state_action_hist])

                # The 'image' in your rollout is already flattened and normalized
                current_image = episode_data[t]['image'] # (224, 224, 3) for a single image
                # reshape to (3, 256, 256) if needed
                if current_image.shape != (256, 256, 3):
                    current_image = cv2.resize(current_image, (256, 256))

                all_data_points.append({
                    "context": {
                        "state_action_history": torch.from_numpy(state_action_hist).float(),
                        "image": torch.from_numpy(current_image.reshape(3, 256, 256)).float(), # Reshape flat image
                        "text": episode_data[t].get('language_instruction', "Default Task Instruction")
                    },
                    "return": returns[t]
                })
        
        print(f"Successfully processed {len(all_data_points)} data points.")
        return all_data_points
    
    @staticmethod
    def _compute_discounted_returns(rewards: List[float], gamma: float) -> torch.Tensor:
        """Calculates the discounted return R_t for each timestep t."""
        returns = []
        future_return = 0.0
        for r in reversed(rewards):
            future_return = r + gamma * future_return
            returns.append(future_return)
        return torch.tensor(list(reversed(returns)), dtype=torch.float32)

    def __len__(self) -> int:
        return len(self.data_points)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        return self.data_points[idx]

# ==============================================================================
# SECTION 2: THE TRAINING SCRIPT
# ==============================================================================

def train_critic_from_rollouts(config: Dict[str, Any]):
    """
    Main function to orchestrate the offline training of the ValueNetwork.
    """
    # --- 1. Setup Environment and Configuration ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # --- 2. Load Data ---
    data_path = Path(config["data_source_path"])
    if not data_path.is_dir():
        raise FileNotFoundError(f"Data source path not found: {data_path}")
    
    # Recursively find all .npy files in the specified directory
    trajectory_files = list(data_path.rglob("*.npy"))
    if not trajectory_files:
        raise FileNotFoundError(f"No .npy files found in {data_path}")

    # Instantiate the dataset
    full_dataset = TrajectoryDataset(trajectory_files, config, history_len=config.get('history_len', 10))
    
    # Split into training and validation sets
    val_split = config.get("val_split_ratio", 0.1)
    val_size = int(len(full_dataset) * val_split)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    print(f"\nDataset loaded: {len(full_dataset)} total samples.")
    print(f"  - Training set size: {len(train_dataset)}")
    print(f"  - Validation set size: {len(val_dataset)}")
    
    # Create DataLoaders
    train_loader = DataLoader(
        train_dataset, batch_size=config["batch_size"], shuffle=True,
        collate_fn=collate_multimodal_batch, num_workers=config.get('num_workers', 4), pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=config["batch_size"], shuffle=False,
        collate_fn=collate_multimodal_batch, num_workers=config.get('num_workers', 4), pin_memory=True
    )
    
    # --- 3. Initialize Model and Optimizer ---
    critic_model = ValueNetwork(config).to(device)

    checkpoint_path = config.get("save_dir", None)
    # check if a pre-trained model exists
    if checkpoint_path:
        checkpoint_path = Path(checkpoint_path) / "critic_best.pth"
        if checkpoint_path.exists():
            print(f"Loading pre-trained model from {checkpoint_path}")
            critic_model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        else:
            print(f"No pre-trained model found at {checkpoint_path}, starting from scratch.")

    optimizer = optim.AdamW(critic_model.parameters(), lr=config.get("lr", 1e-4), weight_decay=1e-3)
    loss_fn = nn.MSELoss()

    # --- 4. Initialize Logging (W&B) ---
    if config.get("use_wandb", False):
        wandb.init(project=config.get("wandb_project", "cal-critic-offline"), config=config)
        wandb.watch(critic_model, log_freq=100)

    # --- 5. Training Loop ---
    best_val_loss = float('inf')
    save_dir = Path(config.get("save_dir", "checkpoints/critic_offline"))
    save_dir.mkdir(parents=True, exist_ok=True)

    print("\n--- Starting Offline Critic Training ---")
    for epoch in range(config.get("epochs", 20)):
        critic_model.train()
        train_loss_total = 0.0
        
        # Training epoch
        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1} [Train]", leave=False):
            context = batch['context']
            target_returns = batch['return'].to(device).unsqueeze(-1)
            
            # Prepare inputs
            state_hist = context['state_action_history'].to(device)
            image = context['image'].to(device)
            text = context['text']
            
            # Forward pass
            predicted_value, _ = critic_model.forward(state_hist, text, image)
            
            # Compute loss
            loss = loss_fn(predicted_value, target_returns)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(critic_model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss_total += loss.item()
        
        avg_train_loss = train_loss_total / len(train_loader)

        # Validation epoch
        critic_model.eval()
        val_loss_total = 0.0
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch + 1} [Val]", leave=False):
                # Similar logic as training, but without backpropagation
                context = batch['context']
                target_returns = batch['return'].to(device).unsqueeze(-1)
                state_hist, image, text = context['state_action_history'].to(device), context['image'].to(device), context['text']
                predicted_value, _ = critic_model.forward(state_hist, text, image)
                loss = loss_fn(predicted_value, target_returns)
                val_loss_total += loss.item()
        
        avg_val_loss = val_loss_total / len(val_loader)
        
        # Logging and Checkpointing
        print(f"Epoch [{epoch + 1:02d}] | Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f}")
        if config.get("use_wandb", False):
            wandb.log({"epoch": epoch + 1, "train_loss": avg_train_loss, "val_loss": avg_val_loss})

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = save_dir / "critic_best.pth"
            torch.save(critic_model.state_dict(), best_model_path)
            print(f"  -> New best model saved to {best_model_path}")
    
    print("\nTraining finished.")
    if config.get("use_wandb", False):
        wandb.finish()


# ==============================================================================
# SECTION 3: SCRIPT EXECUTION
# ==============================================================================

if __name__ == '__main__':
    
    # --- Central Configuration ---
    CONFIG = {
        # Data source path from your rollout script
        "data_source_path": "data/libero/noise",

        # Model Configuration (must match the model used during rollout)
        'state_dim': 8, # Example from LIBERO: 7 for eef_pos+quat_axis_angle, 7 for gripper
        'action_dim': 7, # 6 for eef, 1 for gripper
        'gru_hidden_dim': 256,
        'n_heads': 8,
        'clip_model_name': "openai/clip-vit-base-patch32",
        
        # Reward parameters (should match those used during data collection)
        'reward_alpha': 0.01,
        'reward_beta': 0.1,
        
        # Training Hyperparameters
        "lr": 3e-4,
        "epochs": 500,
        "batch_size": 128,
        "gamma": 0.99, # Discount factor for calculating returns
        "val_split_ratio": 0.15,
        "history_len": 10, # Sequence length for the GRU
        
        # System & Logging
        "num_workers": 4,
        "use_wandb": False, # Set to True to enable logging
        "save_dir": "checkpoints/critic_offline",
    }
    
    # Run the training process
    train_critic_from_rollouts(CONFIG)


